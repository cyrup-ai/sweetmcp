[package]
description="llm_interface: The backend for the llm_client crate"
edition.workspace=true
homepage.workspace=true
license.workspace=true
name="llm_interface"
readme="README.md"
repository.workspace=true
version="0.0.3"

[dependencies]
anyhow.workspace=true
async-stream = "0.3"
backoff = { version = "0.4.0", features = ["tokio"] }
bytes = "1.7.2"
clap = { version = "4.5.17", optional = true }
colorful.workspace=true
dotenvy.workspace=true
futures = "0.3.31"
futures-util = "0.3"
indenter.workspace=true
llm_devices.workspace=true
llm_models.workspace=true
llm_prompt.workspace=true
regex = "1.11.1"
reqwest = { version = "0.12.7", features = ["json", "stream"] }
secrecy = "0.8.0"
serde.workspace=true
serde_json.workspace=true
sysinfo = { version = "0.33.1", optional = true, default-features = false, features = ["system"] }
thiserror.workspace=true
tokio.workspace=true
tokio-util = { version = "0.7", features = ["codec", "io"] } # Added io feature for StreamReader
tracing.workspace=true
url = "2.5.2"

[features]
all=["llama_cpp_backend"]
# all=["llama_cpp_backend", "mistral_rs_backend"]
default=["llama_cpp_backend"]
llama_cpp_backend=["clap", "sysinfo"]
mistral_rs_backend=["sysinfo"]

[dev-dependencies]
serial_test.workspace=true
tokio={workspace=true, features=["macros", "test-util"]}

# [target.'cfg(any(target_os = "linux", target_os = "windows"))'.dependencies]
# mistralrs={git="https://github.com/EricLBuehler/mistral.rs.git", rev="776c11664f36f690937db53cd1809614e64127d4", features=["cuda", "cudnn"]}

# [target.'cfg(target_os = "macos")'.dependencies]
# mistralrs={git="https://github.com/EricLBuehler/mistral.rs.git", rev="776c11664f36f690937db53cd1809614e64127d4", features=["metal"]}

[build-dependencies]
cargo_metadata="0.19.1"
llm_devices.workspace=true

[[bin]]
name="server_runner"
path="src/llms/local/llama_cpp/bin/server_runner.rs"
required-features=["llama_cpp_backend"]

[package.metadata.llama_cpp_backend]
repo="https://github.com/ggml-org/llama.cpp"
tag="b4735"
